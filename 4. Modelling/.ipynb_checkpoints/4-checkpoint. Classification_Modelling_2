{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>set_num</th>\n",
       "      <th>rrp</th>\n",
       "      <th>dollars_per_part</th>\n",
       "      <th>annual_growth</th>\n",
       "      <th>good_investment</th>\n",
       "      <th>years_since_release</th>\n",
       "      <th>num_parts</th>\n",
       "      <th>theme_name</th>\n",
       "      <th>is_parent_theme</th>\n",
       "      <th>num_minifigs</th>\n",
       "      <th>dominant_color</th>\n",
       "      <th>prop_trans_parts</th>\n",
       "      <th>num_unique_parts</th>\n",
       "      <th>num_part_cat</th>\n",
       "      <th>num_cat_part_count</th>\n",
       "      <th>review_score</th>\n",
       "      <th>availability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001-1</td>\n",
       "      <td>6.68</td>\n",
       "      <td>0.155349</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>43</td>\n",
       "      <td>Technic</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>White</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>209</td>\n",
       "      <td>80.0</td>\n",
       "      <td>Retired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0012-1</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.224167</td>\n",
       "      <td>0.126</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>12</td>\n",
       "      <td>Supplemental</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Black</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1414</td>\n",
       "      <td>70.0</td>\n",
       "      <td>Retired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0013-1</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.224167</td>\n",
       "      <td>0.123</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>12</td>\n",
       "      <td>Supplemental</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Black</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1414</td>\n",
       "      <td>62.0</td>\n",
       "      <td>Retired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0014-1</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.224167</td>\n",
       "      <td>0.125</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>12</td>\n",
       "      <td>Supplemental</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Black</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1414</td>\n",
       "      <td>59.0</td>\n",
       "      <td>Retired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0015-1</td>\n",
       "      <td>4.04</td>\n",
       "      <td>0.224444</td>\n",
       "      <td>0.118</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>18</td>\n",
       "      <td>Supplemental</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Black</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1414</td>\n",
       "      <td>77.0</td>\n",
       "      <td>Retired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8670</th>\n",
       "      <td>9916-1</td>\n",
       "      <td>20.24</td>\n",
       "      <td>20.240000</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>Technic</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Green</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>505</td>\n",
       "      <td>82.0</td>\n",
       "      <td>Retired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8671</th>\n",
       "      <td>BLUE-1</td>\n",
       "      <td>6.74</td>\n",
       "      <td>0.143404</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>47</td>\n",
       "      <td>Jurassic World</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sand Green</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>321</td>\n",
       "      <td>75.0</td>\n",
       "      <td>Retired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8672</th>\n",
       "      <td>COMCON034-1</td>\n",
       "      <td>53.99</td>\n",
       "      <td>0.372345</td>\n",
       "      <td>0.209</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>145</td>\n",
       "      <td>Guardians of the Galaxy</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Dark Bluish Gray</td>\n",
       "      <td>0.049180</td>\n",
       "      <td>52</td>\n",
       "      <td>14</td>\n",
       "      <td>313</td>\n",
       "      <td>75.0</td>\n",
       "      <td>Retired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8673</th>\n",
       "      <td>HARLEY-1</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.056197</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>71</td>\n",
       "      <td>Creator Expert</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Light Bluish Gray</td>\n",
       "      <td>0.050633</td>\n",
       "      <td>36</td>\n",
       "      <td>18</td>\n",
       "      <td>427</td>\n",
       "      <td>73.0</td>\n",
       "      <td>Retired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8674</th>\n",
       "      <td>MS1046-1</td>\n",
       "      <td>69.99</td>\n",
       "      <td>69.990000</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>NXT</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[No Color/Any Color]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>505</td>\n",
       "      <td>73.0</td>\n",
       "      <td>Retired</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8675 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          set_num    rrp  dollars_per_part  annual_growth  good_investment  \\\n",
       "0           001-1   6.68          0.155349          0.049                0   \n",
       "1          0012-1   2.69          0.224167          0.126                1   \n",
       "2          0013-1   2.69          0.224167          0.123                1   \n",
       "3          0014-1   2.69          0.224167          0.125                1   \n",
       "4          0015-1   4.04          0.224444          0.118                1   \n",
       "...           ...    ...               ...            ...              ...   \n",
       "8670       9916-1  20.24         20.240000          0.055                0   \n",
       "8671       BLUE-1   6.74          0.143404          0.020                0   \n",
       "8672  COMCON034-1  53.99          0.372345          0.209                1   \n",
       "8673     HARLEY-1   3.99          0.056197          0.000                0   \n",
       "8674     MS1046-1  69.99         69.990000          0.029                0   \n",
       "\n",
       "      years_since_release  num_parts               theme_name  \\\n",
       "0                      56         43                  Technic   \n",
       "1                      42         12             Supplemental   \n",
       "2                      42         12             Supplemental   \n",
       "3                      42         12             Supplemental   \n",
       "4                      42         18             Supplemental   \n",
       "...                   ...        ...                      ...   \n",
       "8670                   22          1                  Technic   \n",
       "8671                    3         47           Jurassic World   \n",
       "8672                    7        145  Guardians of the Galaxy   \n",
       "8673                    2         71           Creator Expert   \n",
       "8674                   14          1                      NXT   \n",
       "\n",
       "      is_parent_theme  num_minifigs        dominant_color  prop_trans_parts  \\\n",
       "0                   1           0.0                 White          0.000000   \n",
       "1                   0           2.0                 Black          0.000000   \n",
       "2                   0           2.0                 Black          0.000000   \n",
       "3                   0           2.0                 Black          0.000000   \n",
       "4                   0           1.0                 Black          0.000000   \n",
       "...               ...           ...                   ...               ...   \n",
       "8670                1           0.0                 Green          0.000000   \n",
       "8671                1           0.0            Sand Green          0.000000   \n",
       "8672                0           1.0      Dark Bluish Gray          0.049180   \n",
       "8673                0           0.0     Light Bluish Gray          0.050633   \n",
       "8674                0           0.0  [No Color/Any Color]          0.000000   \n",
       "\n",
       "      num_unique_parts  num_part_cat  num_cat_part_count  review_score  \\\n",
       "0                   12             5                 209          80.0   \n",
       "1                    1             1                1414          70.0   \n",
       "2                    1             1                1414          62.0   \n",
       "3                    1             1                1414          59.0   \n",
       "4                    1             1                1414          77.0   \n",
       "...                ...           ...                 ...           ...   \n",
       "8670                 1             1                 505          82.0   \n",
       "8671                17            11                 321          75.0   \n",
       "8672                52            14                 313          75.0   \n",
       "8673                36            18                 427          73.0   \n",
       "8674                 1             1                 505          73.0   \n",
       "\n",
       "     availability  \n",
       "0         Retired  \n",
       "1         Retired  \n",
       "2         Retired  \n",
       "3         Retired  \n",
       "4         Retired  \n",
       "...           ...  \n",
       "8670      Retired  \n",
       "8671      Retired  \n",
       "8672      Retired  \n",
       "8673      Retired  \n",
       "8674      Retired  \n",
       "\n",
       "[8675 rows x 17 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/data_for_modelling.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set_num                 object\n",
       "rrp                    float64\n",
       "dollars_per_part       float64\n",
       "annual_growth          float64\n",
       "good_investment          int64\n",
       "years_since_release      int64\n",
       "num_parts                int64\n",
       "theme_name              object\n",
       "is_parent_theme          int64\n",
       "num_minifigs           float64\n",
       "dominant_color          object\n",
       "prop_trans_parts       float64\n",
       "num_unique_parts         int64\n",
       "num_part_cat             int64\n",
       "num_cat_part_count       int64\n",
       "review_score           float64\n",
       "availability            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model contains 11 numerical and 3 categorical variables. It is a binary classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run a simple Logistic Regression to set a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target and features\n",
    "X = df.drop(['set_num','annual_growth','good_investment'], axis = 1)\n",
    "y = df['good_investment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into 80% train and 20% test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.2, random_state = 526)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create encoders for categorical variables 'theme_name', 'dominant_color', and 'availability'\n",
    "theme_train = pd.DataFrame(X_train['theme_name'])\n",
    "theme_test = pd.DataFrame(X_test['theme_name'])\n",
    "\n",
    "color_train = pd.DataFrame(X_train['dominant_color'])\n",
    "color_test = pd.DataFrame(X_test['dominant_color'])\n",
    "\n",
    "ava_train = pd.DataFrame(X_train['availability'])\n",
    "ava_test = pd.DataFrame(X_test['availability'])\n",
    "\n",
    "# Create the encoder\n",
    "ohe_t = OneHotEncoder(handle_unknown='ignore')\n",
    "ohe_t.fit(theme_train)\n",
    "\n",
    "ohe_c = OneHotEncoder(handle_unknown='ignore')\n",
    "ohe_c.fit(color_train)\n",
    "\n",
    "ohe_a = OneHotEncoder(handle_unknown='ignore')\n",
    "ohe_a.fit(ava_train)\n",
    "\n",
    "# Apply the encoder\n",
    "theme_train_ohe = ohe_t.transform(theme_train)\n",
    "theme_test_ohe = ohe_t.transform(theme_test)\n",
    "\n",
    "color_train_ohe = ohe_c.transform(color_train)\n",
    "color_test_ohe = ohe_c.transform(color_test)\n",
    "\n",
    "ava_train_ohe = ohe_a.transform(ava_train)\n",
    "ava_test_ohe = ohe_a.transform(ava_test)\n",
    "\n",
    "# Add pre-fix\n",
    "encoded_theme_train = pd.DataFrame(theme_train_ohe.toarray(), columns=ohe_t.categories_, dtype=int)\n",
    "theme_train_cols = []\n",
    "for column in encoded_theme_train.columns:\n",
    "    column = 'theme_'+column[0]\n",
    "    theme_train_cols.append(column)\n",
    "encoded_theme_train.columns = theme_train_cols\n",
    "    \n",
    "encoded_theme_test = pd.DataFrame(theme_test_ohe.toarray(), columns=ohe_t.categories_, dtype=int)\n",
    "theme_test_cols = []\n",
    "for column in encoded_theme_test.columns:\n",
    "    column = 'theme_'+column[0]\n",
    "    theme_test_cols.append(column)\n",
    "encoded_theme_test.columns = theme_test_cols\n",
    "\n",
    "# Add pre-fix\n",
    "encoded_color_train = pd.DataFrame(color_train_ohe.toarray(), columns=ohe_c.categories_, dtype=int)\n",
    "color_train_cols = []\n",
    "for column in encoded_color_train.columns:\n",
    "    column = 'color_'+column[0]\n",
    "    color_train_cols.append(column)\n",
    "encoded_color_train.columns = color_train_cols\n",
    "\n",
    "encoded_color_test = pd.DataFrame(color_test_ohe.toarray(), columns=ohe_c.categories_, dtype=int)\n",
    "color_test_cols = []\n",
    "for column in encoded_color_test.columns:\n",
    "    column = 'color_'+column[0]\n",
    "    color_test_cols.append(column)\n",
    "encoded_color_test.columns = color_test_cols\n",
    "\n",
    "# Add pre-fix\n",
    "encoded_ava_train = pd.DataFrame(ava_train_ohe.toarray(), columns=ohe_a.categories_, dtype=int)\n",
    "ava_train_cols = []\n",
    "for column in encoded_ava_train.columns:\n",
    "    column = 'availability_'+column[0]\n",
    "    ava_train_cols.append(column)\n",
    "encoded_ava_train.columns = ava_train_cols\n",
    "\n",
    "encoded_ava_test = pd.DataFrame(ava_test_ohe.toarray(), columns=ohe_a.categories_, dtype=int)\n",
    "ava_test_cols = []\n",
    "for column in encoded_ava_test.columns:\n",
    "    column = 'availability_'+column[0]\n",
    "    ava_test_cols.append(column)\n",
    "encoded_ava_test.columns = ava_test_cols\n",
    "\n",
    "# append OneHotEncoded columns back to Train and Test\n",
    "X_train_ohe = pd.concat([X_train.reset_index(drop = True), encoded_theme_train, encoded_color_train, encoded_ava_train], axis =1)\\\n",
    ".drop(['theme_name','dominant_color','availability'], axis = 1)\n",
    "X_test_ohe = pd.concat([X_test.reset_index(drop = True), encoded_theme_test, encoded_color_test, encoded_ava_test], axis =1)\\\n",
    ".drop(['theme_name','dominant_color','availability'], axis = 1)\n",
    "\n",
    "# Scale data\n",
    "ss = StandardScaler().fit(X_train_ohe)\n",
    "\n",
    "X_train_ohe_ss = ss.transform(X_train_ohe)\n",
    "X_test_ohe_ss = ss.transform(X_test_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set accuracy score is 0.726.\n",
      "The training set accuracy score is 0.689.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.74      0.71       907\n",
      "           1       0.69      0.63      0.66       828\n",
      "\n",
      "    accuracy                           0.69      1735\n",
      "   macro avg       0.69      0.69      0.69      1735\n",
      "weighted avg       0.69      0.69      0.69      1735\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "logreg = LogisticRegression().fit(X_train_ohe_ss, y_train)\n",
    "\n",
    "print(f'The training set accuracy score is {logreg.score(X_train_ohe_ss, y_train):,.3f}.')\n",
    "print(f'The training set accuracy score is {logreg.score(X_test_ohe_ss, y_test):,.3f}.')\n",
    "print(classification_report(y_test, logreg.predict(X_test_ohe_ss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple Logistic Regression model without tuning was able to achieve a test accuracy and precision of 0.69. Because this is an investment-inspired problem and I have to put my money on it, I am more concerned about whether the predicted positive is actually a true positive. For that reason, Precision score is chosen for model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is also interpretable. Feature impacts can be inferred from the model coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the coefficients and their corresponding variable names into a dataframe\n",
    "\n",
    "coef_df = pd.DataFrame(zip(X_train_ohe.columns,logreg.coef_[0]), columns=['features', 'coef'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>num_parts</td>\n",
       "      <td>0.555001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>availability_Retired</td>\n",
       "      <td>0.517342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>num_unique_parts</td>\n",
       "      <td>0.324558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is_parent_theme</td>\n",
       "      <td>0.299935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>theme_Classic Space</td>\n",
       "      <td>0.272827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>review_score</td>\n",
       "      <td>0.271309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>theme_Series 19 Minifigures</td>\n",
       "      <td>0.267744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>theme_Series 1</td>\n",
       "      <td>0.256760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>theme_Black Knights</td>\n",
       "      <td>0.256250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>theme_Hero Factory</td>\n",
       "      <td>0.240308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        features      coef\n",
       "3                      num_parts  0.555001\n",
       "496         availability_Retired  0.517342\n",
       "7               num_unique_parts  0.324558\n",
       "4                is_parent_theme  0.299935\n",
       "69           theme_Classic Space  0.272827\n",
       "10                  review_score  0.271309\n",
       "288  theme_Series 19 Minifigures  0.267744\n",
       "277               theme_Series 1  0.256760\n",
       "46           theme_Black Knights  0.256250\n",
       "152           theme_Hero Factory  0.240308"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort the dataframe in a descending order to inpect features with a positive impact\n",
    "\n",
    "coef_df.sort_values('coef', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some of the features that have an positive impact are:\n",
    "- num_parts: in general, larger sets are more investment-worthy\n",
    "- retired status: sets become more valuable when they are no longer available on the primary market\n",
    "- num_unique_parts: unique sets tend to be more desirable and unique sets are built with more unique parts\n",
    "- theme classic_space: sets under the classic space theme tend to be good investment opportunities\n",
    "\n",
    "Concretely, the coefficients can be interpreted as follows:\n",
    "\n",
    "Using num_parts as an example, it has a positive coefficient of 0.555. What this means is that for one unit (1) increase of the number of pieces, the odds of the lego set falling into the \"good investment\" category increases by a factor of exp(0.55) = 1.74, when all other features remain the same. Bear in mind though this does not imply causation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rrp</td>\n",
       "      <td>-0.816601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>availability_Retail</td>\n",
       "      <td>-0.479285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>theme_Bulk Bricks</td>\n",
       "      <td>-0.311093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>theme_Hidden Side</td>\n",
       "      <td>-0.296546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>theme_Universal Building Set</td>\n",
       "      <td>-0.290239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>num_part_cat</td>\n",
       "      <td>-0.229414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>theme_DOTS</td>\n",
       "      <td>-0.227797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>theme_EV3</td>\n",
       "      <td>-0.216039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>theme_Basic</td>\n",
       "      <td>-0.212730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>theme_Discovery</td>\n",
       "      <td>-0.204271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         features      coef\n",
       "0                             rrp -0.816601\n",
       "495           availability_Retail -0.479285\n",
       "59              theme_Bulk Bricks -0.311093\n",
       "154             theme_Hidden Side -0.296546\n",
       "383  theme_Universal Building Set -0.290239\n",
       "8                    num_part_cat -0.229414\n",
       "87                     theme_DOTS -0.227797\n",
       "111                     theme_EV3 -0.216039\n",
       "36                    theme_Basic -0.212730\n",
       "97                theme_Discovery -0.204271"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort the dataframe in an ascending order to inpect features with a negative impact\n",
    "\n",
    "coef_df.sort_values('coef', ascending = True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar interpretations can be applied to these negative features:\n",
    "- rrp: sets with a higher retail price is likely to become a good investment opportunity. This is an interesting finding as num_parts is a positive feature, and larger parts are often more expensive. One should watch for the balance between larger sets and being priced at a reasonable level when picking a lego for investment\n",
    "- available at retail: it is clear that when a set is still available at retail, people's interest in it on the secondary market is low\n",
    "- themes Bulk_Bricks, Universal_Building_Set, Basic: sets under these themes have a lower tendency to generate good returns. Why? I think all these names are simple suggesting a lack of uniqueness. Uniqueness and rarity are the recipe for a good investment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run a base RandomForest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set accuracy score is 1.000.\n",
      "The training set accuracy score is 0.723.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.75      0.74       907\n",
      "           1       0.72      0.69      0.70       828\n",
      "\n",
      "    accuracy                           0.72      1735\n",
      "   macro avg       0.72      0.72      0.72      1735\n",
      "weighted avg       0.72      0.72      0.72      1735\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier().fit(X_train_ohe_ss, y_train)\n",
    "\n",
    "print(f'The training set accuracy score is {rf.score(X_train_ohe_ss, y_train):,.3f}.')\n",
    "print(f'The training set accuracy score is {rf.score(X_test_ohe_ss, y_test):,.3f}.')\n",
    "print(classification_report(y_test, rf.predict(X_test_ohe_ss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame(zip(X_train_ohe.columns,rf.feature_importances_), \n",
    "                                  columns = ['feature_name', 'feature_importance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_name</th>\n",
       "      <th>feature_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>years_since_release</td>\n",
       "      <td>0.082921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>review_score</td>\n",
       "      <td>0.081115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dollars_per_part</td>\n",
       "      <td>0.078677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>num_cat_part_count</td>\n",
       "      <td>0.077944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>num_parts</td>\n",
       "      <td>0.069446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>num_unique_parts</td>\n",
       "      <td>0.065388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rrp</td>\n",
       "      <td>0.060425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>prop_trans_parts</td>\n",
       "      <td>0.055405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>num_part_cat</td>\n",
       "      <td>0.054767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>availability_Retired</td>\n",
       "      <td>0.027917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             feature_name  feature_importance\n",
       "2     years_since_release            0.082921\n",
       "10           review_score            0.081115\n",
       "1        dollars_per_part            0.078677\n",
       "9      num_cat_part_count            0.077944\n",
       "3               num_parts            0.069446\n",
       "7        num_unique_parts            0.065388\n",
       "0                     rrp            0.060425\n",
       "6        prop_trans_parts            0.055405\n",
       "8            num_part_cat            0.054767\n",
       "496  availability_Retired            0.027917"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance.sort_values('feature_importance', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_name</th>\n",
       "      <th>feature_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>theme_Mindstorms</td>\n",
       "      <td>9.069011e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>theme_Trolls: World Tour</td>\n",
       "      <td>1.585808e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>color_Light Violet</td>\n",
       "      <td>1.697958e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>theme_Disney</td>\n",
       "      <td>1.842916e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>theme_Stranger Things</td>\n",
       "      <td>2.631741e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>theme_Barraki</td>\n",
       "      <td>2.941902e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>theme_Frozen II</td>\n",
       "      <td>2.975306e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>availability_LEGOLAND Exclusive</td>\n",
       "      <td>3.010061e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>color_Trans-Purple</td>\n",
       "      <td>5.545292e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>theme_Rattles</td>\n",
       "      <td>6.682482e-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        feature_name  feature_importance\n",
       "207                 theme_Mindstorms        9.069011e-07\n",
       "375         theme_Trolls: World Tour        1.585808e-06\n",
       "442               color_Light Violet        1.697958e-06\n",
       "98                      theme_Disney        1.842916e-06\n",
       "329            theme_Stranger Things        2.631741e-06\n",
       "35                     theme_Barraki        2.941902e-06\n",
       "134                  theme_Frozen II        2.975306e-06\n",
       "493  availability_LEGOLAND Exclusive        3.010061e-06\n",
       "482               color_Trans-Purple        5.545292e-06\n",
       "264                    theme_Rattles        6.682482e-06"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance.sort_values('feature_importance', ascending = True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three groups of classification models with different complexities will be tested and their hyperparameters will be tuned before their performances can be compared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Group 1: Standalone Classifiers\n",
    "This modelling group includes the following models:\n",
    "- Logistic Regression\n",
    "- Support Vector Machine (Linear and Non-linear)\n",
    "- Naive Bayes\n",
    "- K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 22 candidates, totalling 110 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1881s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done  44 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=-1)]: Done  88 tasks      | elapsed:    8.7s\n",
      "[Parallel(n_jobs=-1)]: Done 103 out of 110 | elapsed:    9.7s remaining:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 110 out of 110 | elapsed:   10.3s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tempfile import mkdtemp\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "cachedir = mkdtemp()\n",
    "\n",
    "# Set up the ML Pipeline\n",
    "\n",
    "# numerical features\n",
    "numeric_features = ['rrp', 'dollars_per_part','years_since_release', 'num_parts', 'is_parent_theme', 'num_minifigs',\n",
    "       'prop_trans_parts', 'num_unique_parts', 'num_part_cat', 'num_cat_part_count', 'review_score']\n",
    "numeric_transformer = StandardScaler()\n",
    "\n",
    "# categorical features\n",
    "categorical_features = ['theme_name', 'dominant_color', 'availability']\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers = [('num', numeric_transformer, numeric_features),\n",
    "                                                 ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# prediction pipeline\n",
    "\n",
    "clf = Pipeline(steps = [('preprocessor', preprocessor),\n",
    "                       ('classifier', LogisticRegression())])\n",
    "\n",
    "param_grid = [\n",
    "    {'classifier': [LogisticRegression(solver = 'lbfgs')],\n",
    "    'classifier__C': list(np.logspace(-5,5,11)),\n",
    "    'classifier__penalty': ['l1','l2']} \n",
    "]\n",
    "\n",
    "# Set precision as the evaluation metric and perform a 5-fold validation\n",
    "\n",
    "grid = GridSearchCV(clf, param_grid=param_grid, scoring = 'precision',cv=5, n_jobs=-1, verbose=10)\n",
    "\n",
    "fitted_grid = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7220285261489698\n",
      "0.7003994673768309\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.75      0.72       907\n",
      "           1       0.70      0.64      0.67       828\n",
      "\n",
      "    accuracy                           0.70      1735\n",
      "   macro avg       0.70      0.69      0.69      1735\n",
      "weighted avg       0.70      0.70      0.70      1735\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(fitted_grid.score(X_train, y_train))\n",
    "print(fitted_grid.score(X_test, y_test))\n",
    "print(classification_report(y_test, fitted_grid.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(C=10.0),\n",
       " 'classifier__C': 10.0,\n",
       " 'classifier__penalty': 'l2'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best performing Logistic Regression model was able to achieve a precision score of 0.70, a slight increase from its original form. The best model uses L2 (Ridge) as the penalty term and the magnitude of regularization is 1/C = 0.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-2 Support Vector Machine (Linear and Non-linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 11 candidates, totalling 55 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:   19.7s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   30.2s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   48.0s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   58.5s\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done  55 out of  55 | elapsed:  4.8min finished\n"
     ]
    }
   ],
   "source": [
    "# Non-linear model with a rbf kernel\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "cachedir = mkdtemp()\n",
    "\n",
    "# prediction pipeline\n",
    "\n",
    "clf = Pipeline(steps = [('preprocessor', preprocessor),\n",
    "                       ('classifier', SVC())])\n",
    "param_grid = [\n",
    "    {'classifier': [SVC()],\n",
    "    'classifier__C': list(np.logspace(-5,5,11)),\n",
    "    'classifier__kernel': ['rbf']}\n",
    "]\n",
    "\n",
    "# Set precision as the evaluation metric and perform a 5-fold validation\n",
    "\n",
    "grid = GridSearchCV(clf, param_grid=param_grid, scoring = 'precision',cv=5, n_jobs=-1, verbose=10)\n",
    "\n",
    "fitted_grid = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8205596107055961\n",
      "0.7368421052631579\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': SVC(C=10.0), 'classifier__C': 10.0, 'classifier__kernel': 'rbf'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(fitted_grid.score(X_train, y_train))\n",
    "print(fitted_grid.score(X_test, y_test))\n",
    "fitted_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try further optimizing the model in a tighter C value range and also add gamma to the search grid. The gamma parameter implies the radius of influence of samples selected by the model as support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 38 candidates, totalling 190 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:   20.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   29.2s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   46.1s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   58.4s\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=-1)]: Done 137 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=-1)]: Done 173 tasks      | elapsed:  7.8min\n",
      "[Parallel(n_jobs=-1)]: Done 190 out of 190 | elapsed:  8.6min finished\n"
     ]
    }
   ],
   "source": [
    "# prediction pipeline\n",
    "\n",
    "clf = Pipeline(steps = [('preprocessor', preprocessor),\n",
    "                       ('classifier', SVC())])\n",
    "param_grid = [\n",
    "    {'classifier': [SVC()],\n",
    "    'classifier__C': range(1,20),\n",
    "    'classifier__kernel': ['rbf'],\n",
    "    'classifier__gamma': ['auto', 'scale']},\n",
    "]\n",
    "\n",
    "# Set precision as the evaluation metric and perform a 5-fold validation\n",
    "\n",
    "grid = GridSearchCV(clf, param_grid=param_grid, scoring = 'precision',cv=5, n_jobs=-1, verbose=10)\n",
    "\n",
    "fitted_grid = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7921960072595281\n",
      "0.7295918367346939\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.77      0.75       907\n",
      "           1       0.73      0.69      0.71       828\n",
      "\n",
      "    accuracy                           0.73      1735\n",
      "   macro avg       0.73      0.73      0.73      1735\n",
      "weighted avg       0.73      0.73      0.73      1735\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': SVC(C=5),\n",
       " 'classifier__C': 5,\n",
       " 'classifier__gamma': 'scale',\n",
       " 'classifier__kernel': 'rbf'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(fitted_grid.score(X_train, y_train))\n",
    "print(fitted_grid.score(X_test, y_test))\n",
    "print(classification_report(y_test, fitted_grid.predict(X_test)))\n",
    "fitted_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This optimization failed to further improve the model performance. \n",
    "\n",
    "The best performing non-linear Support Vector Machine model was able to achieve a precision score of 0.74, a sizable improvement from the baseline Logistic Regression model. The best performing model uses the rbf kernel and has a C value of 10. The C value regulates the balance between correct classification of training examples and maximization of the decision functionâ€™s margin.\n",
    "\n",
    "One downside of this good performing model is its lack of interpretability. It lacks interpretability because of its implicit transformation to the higher dimensional space. A framework called Recursive Feature Elimination (SVM-RBF-RFE) has been mentioned in the literature and will be studied further in order to also draw insights from the features used in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1510s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too slow (2.1428s.) Setting batch_size=1.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done  36 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:    9.3s\n",
      "[Parallel(n_jobs=-1)]: Done  54 tasks      | elapsed:   11.6s\n",
      "[Parallel(n_jobs=-1)]: Done  70 out of  70 | elapsed:   16.2s finished\n"
     ]
    }
   ],
   "source": [
    "# Run a linear model\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "cachedir = mkdtemp()\n",
    "\n",
    "# prediction pipeline\n",
    "\n",
    "clf = Pipeline(steps = [('preprocessor', preprocessor),\n",
    "                       ('classifier', LinearSVC())])\n",
    "param_grid = [\n",
    "    {'classifier': [LinearSVC()],\n",
    "    'classifier__C': [0.1,0.3,0.5,0.8,1,1.2,1.5],\n",
    "    'classifier__penalty': ['l2'],\n",
    "    'classifier__loss': ['hinge', 'squared_hinge']}\n",
    "]\n",
    "\n",
    "# Set precision as the evaluation metric and perform a 5-fold validation\n",
    "\n",
    "grid = GridSearchCV(clf, param_grid=param_grid, scoring = 'precision',cv=5, n_jobs=-1, verbose=10)\n",
    "\n",
    "fitted_grid = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7188767550702028\n",
      "0.6972477064220184\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.75      0.72       907\n",
      "           1       0.70      0.64      0.67       828\n",
      "\n",
      "    accuracy                           0.70      1735\n",
      "   macro avg       0.70      0.69      0.69      1735\n",
      "weighted avg       0.70      0.70      0.70      1735\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': LinearSVC(C=0.3),\n",
       " 'classifier__C': 0.3,\n",
       " 'classifier__loss': 'squared_hinge',\n",
       " 'classifier__penalty': 'l2'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(fitted_grid.score(X_train, y_train))\n",
    "print(fitted_grid.score(X_test, y_test))\n",
    "print(classification_report(y_test, fitted_grid.predict(X_test)))\n",
    "fitted_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear support vector machine model was able to achieve a precision score of 0.70 with an L2 penalty term and C value of 0.3. The best loss function is the default squred_hinge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being a linear model, the impact of the features can also be inferred from the model coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = LinearSVC(C = 0.3).fit(X_train_ohe_ss, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_svc_l = pd.DataFrame(zip(X_train_ohe.columns,svc.coef_[0]), columns=['features', 'coef'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>num_parts</td>\n",
       "      <td>0.219276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>availability_Retired</td>\n",
       "      <td>0.178102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is_parent_theme</td>\n",
       "      <td>0.132481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>num_unique_parts</td>\n",
       "      <td>0.126618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>review_score</td>\n",
       "      <td>0.107410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>theme_Classic Space</td>\n",
       "      <td>0.098013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>num_cat_part_count</td>\n",
       "      <td>0.088945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>theme_Heroes</td>\n",
       "      <td>0.057735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>theme_Series 1</td>\n",
       "      <td>0.047453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>theme_Villains</td>\n",
       "      <td>0.047368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 features      coef\n",
       "3               num_parts  0.219276\n",
       "496  availability_Retired  0.178102\n",
       "4         is_parent_theme  0.132481\n",
       "7        num_unique_parts  0.126618\n",
       "10           review_score  0.107410\n",
       "69    theme_Classic Space  0.098013\n",
       "9      num_cat_part_count  0.088945\n",
       "153          theme_Heroes  0.057735\n",
       "277        theme_Series 1  0.047453\n",
       "390        theme_Villains  0.047368"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_svc_l.sort_values('coef', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rrp</td>\n",
       "      <td>-0.322785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>availability_Retail</td>\n",
       "      <td>-0.160611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>theme_Bulk Bricks</td>\n",
       "      <td>-0.108568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>num_part_cat</td>\n",
       "      <td>-0.089895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>theme_Hidden Side</td>\n",
       "      <td>-0.073012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>theme_Legends of Chima</td>\n",
       "      <td>-0.060570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>availability_Exclusive</td>\n",
       "      <td>-0.058038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>theme_Dimensions</td>\n",
       "      <td>-0.055698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>theme_Service Packs</td>\n",
       "      <td>-0.054404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>theme_Scala</td>\n",
       "      <td>-0.052419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   features      coef\n",
       "0                       rrp -0.322785\n",
       "495     availability_Retail -0.160611\n",
       "59        theme_Bulk Bricks -0.108568\n",
       "8              num_part_cat -0.089895\n",
       "154       theme_Hidden Side -0.073012\n",
       "189  theme_Legends of Chima -0.060570\n",
       "492  availability_Exclusive -0.058038\n",
       "91         theme_Dimensions -0.055698\n",
       "306     theme_Service Packs -0.054404\n",
       "273             theme_Scala -0.052419"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_svc_l.sort_values('coef', ascending = True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has identified similar features as the Logistic Regression for impacting the classification. However, their interpretations are different. In a linear SVM model, the result is a hyperplane that separates the classes. The coefficients here represents a vector that is orthogonal to the hyperplane. Their sign gives the direction of the predicted class and the magnitude of their values is an indication of the feature importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-3 Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes is a different classification framework than frequentists'. A Gaussian distribution is commonly assumed and a simple model is tested below for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set accuracy score is 0.6777.\n",
      "The test set accuracy score is 0.6248.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.84      0.70       907\n",
      "           1       0.69      0.39      0.50       828\n",
      "\n",
      "    accuracy                           0.62      1735\n",
      "   macro avg       0.65      0.61      0.60      1735\n",
      "weighted avg       0.64      0.62      0.60      1735\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "NB = GaussianNB().fit(X_train_ohe_ss, y_train)\n",
    "\n",
    "print(f'The training set accuracy score is {NB.score(X_train_ohe_ss, y_train):,.4f}.')\n",
    "print(f'The test set accuracy score is {NB.score(X_test_ohe_ss, y_test):,.4f}.')\n",
    "print(classification_report(y_test, NB.predict(X_test_ohe_ss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gaussian Naive Bayes classifier was able to achieve a precision score pf 0.69. Although the accuracy score is not the evaluation metric of this project, it is noticeably lower than those from the previous models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-4 K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:   12.6s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   14.3s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   17.3s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   21.0s\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   25.0s\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   28.6s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   32.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = Pipeline(steps = [('preprocessor', preprocessor),\n",
    "                       ('classifier', LogisticRegression())])\n",
    "\n",
    "param_grid = [\n",
    "   {'classifier': [KNeighborsClassifier()],\n",
    "    'classifier__n_neighbors': range(1,10,2),\n",
    "    'classifier__weights': ['uniform','distance']}\n",
    "]\n",
    "\n",
    "# Set precision as the evaluation metric and perform a 5-fold validation\n",
    "\n",
    "grid = GridSearchCV(clf, param_grid=param_grid, scoring = 'precision', cv=5, n_jobs=-1, verbose=10)\n",
    "\n",
    "fitted_grid = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9996979764421625\n",
      "0.6596794081381011\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.70      0.69       907\n",
      "           1       0.66      0.65      0.65       828\n",
      "\n",
      "    accuracy                           0.67      1735\n",
      "   macro avg       0.67      0.67      0.67      1735\n",
      "weighted avg       0.67      0.67      0.67      1735\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': KNeighborsClassifier(n_neighbors=7, weights='distance'),\n",
       " 'classifier__n_neighbors': 7,\n",
       " 'classifier__weights': 'distance'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(fitted_grid.score(X_train, y_train))\n",
    "print(fitted_grid.score(X_test, y_test))\n",
    "print(classification_report(y_test, fitted_grid.predict(X_test)))\n",
    "fitted_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors (KNN) is one of the \"lazy-learning\" and most intuitive models. However, in practice it rarely is the best performing model. For this project, KNN was able to achieve a precision score of 0.66 with 7 distance-weighted neighbors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group 2: Ensemble Models\n",
    "\n",
    "Two types of ensemble models have been tested in this project\n",
    "\n",
    "- Bagging: Random Forest\n",
    "- Boosting: XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-1 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 360 candidates, totalling 1800 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   21.3s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   25.0s\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   32.2s\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   43.5s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:   52.5s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 137 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 173 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 234 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 257 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 305 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done 330 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  6.3min\n",
      "[Parallel(n_jobs=-1)]: Done 384 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=-1)]: Done 413 tasks      | elapsed:  7.8min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  8.7min\n",
      "[Parallel(n_jobs=-1)]: Done 473 tasks      | elapsed:  9.6min\n",
      "[Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed: 10.4min\n",
      "[Parallel(n_jobs=-1)]: Done 537 tasks      | elapsed: 11.2min\n",
      "[Parallel(n_jobs=-1)]: Done 570 tasks      | elapsed: 12.3min\n",
      "[Parallel(n_jobs=-1)]: Done 605 tasks      | elapsed: 13.4min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed: 14.4min\n",
      "[Parallel(n_jobs=-1)]: Done 677 tasks      | elapsed: 15.6min\n",
      "[Parallel(n_jobs=-1)]: Done 714 tasks      | elapsed: 16.5min\n",
      "[Parallel(n_jobs=-1)]: Done 753 tasks      | elapsed: 18.9min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed: 21.1min\n",
      "[Parallel(n_jobs=-1)]: Done 833 tasks      | elapsed: 22.4min\n",
      "[Parallel(n_jobs=-1)]: Done 874 tasks      | elapsed: 23.6min\n",
      "[Parallel(n_jobs=-1)]: Done 917 tasks      | elapsed: 25.4min\n",
      "[Parallel(n_jobs=-1)]: Done 960 tasks      | elapsed: 27.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1005 tasks      | elapsed: 29.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1050 tasks      | elapsed: 30.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1097 tasks      | elapsed: 32.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1144 tasks      | elapsed: 35.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1193 tasks      | elapsed: 37.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed: 38.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1293 tasks      | elapsed: 41.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1344 tasks      | elapsed: 44.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1397 tasks      | elapsed: 46.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1450 tasks      | elapsed: 48.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1505 tasks      | elapsed: 52.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1560 tasks      | elapsed: 54.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1617 tasks      | elapsed: 55.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1674 tasks      | elapsed: 59.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1733 tasks      | elapsed: 61.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed: 63.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1800 out of 1800 | elapsed: 64.0min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "cachedir = mkdtemp()\n",
    "\n",
    "# re-set up the ML Pipeline as feature scaling is not required for random forest models\n",
    "\n",
    "# categorical features\n",
    "categorical_features = ['theme_name', 'dominant_color', 'availability']\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers = [('cat', categorical_transformer, categorical_features)], \n",
    "                                 remainder='passthrough')\n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', RandomForestClassifier())])\n",
    "\n",
    "param_grid = [\n",
    " \n",
    "    {'classifier': [RandomForestClassifier()],\n",
    "    'classifier__n_estimators': [100, 200, 500, 1000],\n",
    "    'classifier__max_depth': [10, 20, 40, 70, None],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4],\n",
    "    'classifier__max_features': ['auto', 'sqrt'],\n",
    "    'classifier__min_samples_split': [2, 5, 10]}\n",
    "\n",
    "]\n",
    "\n",
    "grid = GridSearchCV(clf, param_grid=param_grid, scoring = 'precision', cv=5, n_jobs=-1, verbose=10)\n",
    "\n",
    "fitted_grid = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.950834597875569\n",
      "0.7232704402515723\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.76      0.74       907\n",
      "           1       0.72      0.69      0.71       828\n",
      "\n",
      "    accuracy                           0.73      1735\n",
      "   macro avg       0.73      0.73      0.73      1735\n",
      "weighted avg       0.73      0.73      0.73      1735\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': RandomForestClassifier(max_depth=70, max_features='sqrt', min_samples_split=10,\n",
       "                        n_estimators=200),\n",
       " 'classifier__max_depth': 70,\n",
       " 'classifier__max_features': 'sqrt',\n",
       " 'classifier__min_samples_leaf': 1,\n",
       " 'classifier__min_samples_split': 10,\n",
       " 'classifier__n_estimators': 200}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(fitted_grid.score(X_train, y_train))\n",
    "print(fitted_grid.score(X_test, y_test))\n",
    "print(classification_report(y_test, fitted_grid.predict(X_test)))\n",
    "fitted_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RandomForest model usually works fairly well with just default parameters. Through optimization, it was able to achieve a precision of 0.72. Each tree is expected to overfit but overall the overfitting would have been offset by the variance of large number of trees.\n",
    "\n",
    "Another advantage of the RandomForest model is its interpretability of the feature importance as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(max_depth=70, max_features='sqrt', min_samples_split=10,\n",
    "                        n_estimators=200).fit(X_train_ohe, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame(zip(X_train_ohe.columns,rf.feature_importances_), \n",
    "                                  columns = ['feature_name', 'feature_importance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_name</th>\n",
       "      <th>feature_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>years_since_release</td>\n",
       "      <td>0.085205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>num_cat_part_count</td>\n",
       "      <td>0.068027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dollars_per_part</td>\n",
       "      <td>0.066364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>review_score</td>\n",
       "      <td>0.062775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>num_parts</td>\n",
       "      <td>0.057421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rrp</td>\n",
       "      <td>0.054762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>num_unique_parts</td>\n",
       "      <td>0.054072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>num_part_cat</td>\n",
       "      <td>0.047084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>prop_trans_parts</td>\n",
       "      <td>0.045668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>availability_Retired</td>\n",
       "      <td>0.038478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             feature_name  feature_importance\n",
       "2     years_since_release            0.085205\n",
       "9      num_cat_part_count            0.068027\n",
       "1        dollars_per_part            0.066364\n",
       "10           review_score            0.062775\n",
       "3               num_parts            0.057421\n",
       "0                     rrp            0.054762\n",
       "7        num_unique_parts            0.054072\n",
       "8            num_part_cat            0.047084\n",
       "6        prop_trans_parts            0.045668\n",
       "496  availability_Retired            0.038478"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance.sort_values('feature_importance', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has identified the following features to be important for class prediction:\n",
    "- years_since_release. As indicated in when the feature was engineered, it was expected to have a negative impact on the investment. This might be interpreted in two different ways: 1. Holding a set for a longer time tends to reduce the average annual returns. 2. Sets that have been released recently may indeed be better investments than those that were released earlier due to other features.\n",
    "- num_cat_part_count. This feature is one of the measures of how unique each set is. Similar to the linear models, this type of features tend to have a bigger impact on the prediction.\n",
    "- dollars_per_part and review_score. It is encouraging to see that these engineered features, as detailed in the \"Exploratory Data Analysis (EDA)\" notebook, are making an impact to the model prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_name</th>\n",
       "      <th>feature_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>availability_LEGOLAND Exclusive</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>theme_Minions</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>theme_Mindstorms</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>theme_Frozen II</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>theme_Stranger Things</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>theme_Barraki</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>theme_Series 20 Minifigures</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>color_Trans-Light Royal Blue</td>\n",
       "      <td>0.000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>theme_Creature</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>color_Light Salmon</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        feature_name  feature_importance\n",
       "493  availability_LEGOLAND Exclusive            0.000003\n",
       "210                    theme_Minions            0.000003\n",
       "207                 theme_Mindstorms            0.000005\n",
       "134                  theme_Frozen II            0.000005\n",
       "329            theme_Stranger Things            0.000006\n",
       "35                     theme_Barraki            0.000006\n",
       "291      theme_Series 20 Minifigures            0.000010\n",
       "477     color_Trans-Light Royal Blue            0.000011\n",
       "81                    theme_Creature            0.000013\n",
       "441               color_Light Salmon            0.000015"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance.sort_values('feature_importance', ascending = True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForest has identified a number of theme features as being less relevant to prediction. This has not been tested in the previous linear models, but in there these would be equivalent to the features having an associated coefficient close to 0, positive or negative. In the future, RandomForest will be considered, alongside other dimensionality reduction techniques, to reduce features and simplify models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-2 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    6.9s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    7.4s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:    9.3s\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   10.0s\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   11.7s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:   14.2s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:   16.2s\n",
      "[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed:   19.1s\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:   22.1s\n",
      "[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed:   26.1s\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:   31.3s\n",
      "[Parallel(n_jobs=-1)]: Done 137 tasks      | elapsed:   36.3s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   43.2s\n",
      "[Parallel(n_jobs=-1)]: Done 173 tasks      | elapsed:   53.0s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 234 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 257 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 305 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 330 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 384 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 413 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 473 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 537 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 570 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 605 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 677 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 714 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 753 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 833 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 874 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done 917 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done 960 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  6.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:46:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "cachedir = mkdtemp()\n",
    "\n",
    "# Similar to RandomForest, XGBoost is a tree-based model and therefore it does not require feature scaling\n",
    "\n",
    "# categorical features\n",
    "categorical_features = ['theme_name', 'dominant_color', 'availability']\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers = [('cat', categorical_transformer, categorical_features)], \n",
    "                                 remainder='passthrough')\n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', XGBClassifier())])\n",
    "\n",
    "param_grid = [\n",
    " \n",
    "    {'classifier': [XGBClassifier()],\n",
    "    'classifier__n_estimators': np.arange(1, 101, 20),\n",
    "    'classifier__max_depth': np.arange(1, 20, 2),\n",
    "    'classifier__learning_rate': [0.10, 0.20, 0.3, 0.5]}\n",
    "]\n",
    "\n",
    "grid = GridSearchCV(clf, param_grid=param_grid, scoring = 'precision', cv=5, n_jobs=-1, verbose=10)\n",
    "\n",
    "fitted_grid = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8430467397576457\n",
      "0.6855123674911661\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.71      0.71       907\n",
      "           1       0.69      0.70      0.69       828\n",
      "\n",
      "    accuracy                           0.70      1735\n",
      "   macro avg       0.70      0.70      0.70      1735\n",
      "weighted avg       0.70      0.70      0.70      1735\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
       "               colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
       "               gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
       "               learning_rate=0.2, max_delta_step=None, max_depth=9,\n",
       "               min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "               n_estimators=81, n_jobs=None, num_parallel_tree=None,\n",
       "               random_state=None, reg_alpha=None, reg_lambda=None,\n",
       "               scale_pos_weight=None, subsample=None, tree_method=None,\n",
       "               validate_parameters=None, verbosity=None),\n",
       " 'classifier__learning_rate': 0.2,\n",
       " 'classifier__max_depth': 9,\n",
       " 'classifier__n_estimators': 81}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(fitted_grid.score(X_train, y_train))\n",
    "print(fitted_grid.score(X_test, y_test))\n",
    "print(classification_report(y_test, fitted_grid.predict(X_test)))\n",
    "fitted_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost is widely considered as one of the most powerful models. As an boosting type ensemble model, each iteration in sequence puts more emphasis on the samples that have been miss-classified in the previous iteration. It, however, did not achieve a level of performance as I expected in this project. The best XGBoost model had a precision score of 0.69, making it inferior to the RandomForest model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group 3: Deep Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-1 Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some simple neural network models have also been tested. First construct a simple 2-layer Feedforward Neural Network with the first layer having roughly the same number of neurons as the number of features in the dataset. After onehotencoding, the dataset has 497 features.\n",
    "\n",
    "Apply regularization and neuron dropout to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 1.9719 - precision_3: 0.6307\n",
      "Epoch 2/50\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.8709 - precision_3: 0.6876\n",
      "Epoch 3/50\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.6847 - precision_3: 0.6928\n",
      "Epoch 4/50\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.6431 - precision_3: 0.7003\n",
      "Epoch 5/50\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.6282 - precision_3: 0.6993\n",
      "Epoch 6/50\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.6207 - precision_3: 0.7030\n",
      "Epoch 7/50\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.6112 - precision_3: 0.7000\n",
      "Epoch 8/50\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.6120 - precision_3: 0.6969\n",
      "Epoch 9/50\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.6021 - precision_3: 0.7071\n",
      "Epoch 10/50\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5960 - precision_3: 0.7048\n",
      "Epoch 11/50\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5958 - precision_3: 0.7181\n",
      "Epoch 12/50\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5858 - precision_3: 0.7235\n",
      "Epoch 13/50\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5848 - precision_3: 0.7135\n",
      "Epoch 14/50\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5785 - precision_3: 0.7188\n",
      "Epoch 15/50\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5746 - precision_3: 0.7163\n",
      "Epoch 16/50\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5697 - precision_3: 0.7235\n",
      "Epoch 17/50\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5715 - precision_3: 0.7211\n",
      "Epoch 18/50\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5674 - precision_3: 0.7305A: 0s - loss: 0.5607 - prec\n",
      "Epoch 19/50\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5640 - precision_3: 0.7308\n",
      "Epoch 20/50\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5628 - precision_3: 0.7268\n",
      "Epoch 21/50\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5597 - precision_3: 0.7330\n",
      "Epoch 22/50\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5588 - precision_3: 0.7259\n",
      "Epoch 23/50\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5551 - precision_3: 0.7304\n",
      "Epoch 24/50\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5544 - precision_3: 0.7324\n",
      "Epoch 25/50\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5539 - precision_3: 0.7399\n",
      "Epoch 26/50\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5510 - precision_3: 0.7351\n",
      "Epoch 27/50\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5483 - precision_3: 0.7369\n",
      "Epoch 28/50\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5441 - precision_3: 0.7394\n",
      "Epoch 29/50\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5479 - precision_3: 0.7402\n",
      "Epoch 30/50\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5481 - precision_3: 0.7412\n",
      "Epoch 31/50\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5459 - precision_3: 0.7484\n",
      "Epoch 32/50\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5442 - precision_3: 0.7404\n",
      "Epoch 33/50\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5424 - precision_3: 0.7357\n",
      "Epoch 34/50\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5415 - precision_3: 0.7414\n",
      "Epoch 35/50\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5408 - precision_3: 0.7411\n",
      "Epoch 36/50\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5392 - precision_3: 0.7381A: 0s - loss: 0.5212 - p\n",
      "Epoch 37/50\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5327 - precision_3: 0.7473\n",
      "Epoch 38/50\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5373 - precision_3: 0.7565\n",
      "Epoch 39/50\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5344 - precision_3: 0.7475A: 1s - loss:\n",
      "Epoch 40/50\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5393 - precision_3: 0.7441\n",
      "Epoch 41/50\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5373 - precision_3: 0.7394\n",
      "Epoch 42/50\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5356 - precision_3: 0.7435\n",
      "Epoch 43/50\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5311 - precision_3: 0.7530\n",
      "Epoch 44/50\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5293 - precision_3: 0.7483\n",
      "Epoch 45/50\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5315 - precision_3: 0.7503\n",
      "Epoch 46/50\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5257 - precision_3: 0.7546\n",
      "Epoch 47/50\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5318 - precision_3: 0.7529\n",
      "Epoch 48/50\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5277 - precision_3: 0.7469\n",
      "Epoch 49/50\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5301 - precision_3: 0.7515\n",
      "Epoch 50/50\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5291 - precision_3: 0.7530\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Create a new sequential model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Create a regularizer with a factor of 0.005 and apply it to all hidden layers\n",
    "regularizer = keras.regularizers.l2(0.005)\n",
    "\n",
    "# Declare the hidden layers and set the # of neurons in the first layer to roughly macth the number of input variables\n",
    "model.add(layers.Dense(500, activation=\"relu\", kernel_regularizer=regularizer))\n",
    "model.add(layers.Dropout(0.2)) # Set 20% of the nodes to 0. \n",
    "model.add(layers.Dense(250, activation=\"relu\", kernel_regularizer=regularizer))\n",
    "model.add(layers.Dropout(0.2)) # Set 20% of the nodes to 0. \n",
    "\n",
    "# Declare the output layer\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the model and leave learning rate as default. Set precision as the evaluation metric.\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[keras.metrics.Precision()]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_ohe_ss, \n",
    "                    y_train, \n",
    "                    epochs=50, \n",
    "                    verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Precision: 0.7530\n",
      "Test Precision: 0.7068\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the network\n",
    "train_precision = history.history['precision_3'][-1]\n",
    "result = model.evaluate(X_test_ohe_ss, y_test, verbose=0)\n",
    "\n",
    "print(f\"Train Precision: {train_precision:.4f}\")\n",
    "print(f\"Test Precision: {result[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a 3rd layer to increase the complexity of its architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "217/217 [==============================] - 2s 10ms/step - loss: 2.3317 - precision_8: 0.6260\n",
      "Epoch 2/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.8835 - precision_8: 0.6920\n",
      "Epoch 3/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.6870 - precision_8: 0.7018\n",
      "Epoch 4/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.6404 - precision_8: 0.6982\n",
      "Epoch 5/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.6254 - precision_8: 0.7102\n",
      "Epoch 6/150\n",
      "217/217 [==============================] - 2s 10ms/step - loss: 0.6224 - precision_8: 0.7060\n",
      "Epoch 7/150\n",
      "217/217 [==============================] - 3s 12ms/step - loss: 0.6103 - precision_8: 0.7053\n",
      "Epoch 8/150\n",
      "217/217 [==============================] - 2s 10ms/step - loss: 0.6133 - precision_8: 0.7083\n",
      "Epoch 9/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.6008 - precision_8: 0.7114\n",
      "Epoch 10/150\n",
      "217/217 [==============================] - 2s 10ms/step - loss: 0.5996 - precision_8: 0.7074\n",
      "Epoch 11/150\n",
      "217/217 [==============================] - 2s 10ms/step - loss: 0.5925 - precision_8: 0.7137\n",
      "Epoch 12/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5918 - precision_8: 0.7131\n",
      "Epoch 13/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5866 - precision_8: 0.7178\n",
      "Epoch 14/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5826 - precision_8: 0.7248\n",
      "Epoch 15/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5789 - precision_8: 0.7225\n",
      "Epoch 16/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5765 - precision_8: 0.7216\n",
      "Epoch 17/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5749 - precision_8: 0.7250\n",
      "Epoch 18/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5699 - precision_8: 0.7309\n",
      "Epoch 19/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5712 - precision_8: 0.7274\n",
      "Epoch 20/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5668 - precision_8: 0.7252\n",
      "Epoch 21/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5699 - precision_8: 0.7215\n",
      "Epoch 22/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5647 - precision_8: 0.7332\n",
      "Epoch 23/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5668 - precision_8: 0.7346\n",
      "Epoch 24/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5658 - precision_8: 0.7375\n",
      "Epoch 25/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5590 - precision_8: 0.7334\n",
      "Epoch 26/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5608 - precision_8: 0.7364\n",
      "Epoch 27/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5596 - precision_8: 0.7381\n",
      "Epoch 28/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5533 - precision_8: 0.7404\n",
      "Epoch 29/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5582 - precision_8: 0.7369\n",
      "Epoch 30/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5537 - precision_8: 0.7375\n",
      "Epoch 31/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5522 - precision_8: 0.7456\n",
      "Epoch 32/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5510 - precision_8: 0.7336\n",
      "Epoch 33/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5540 - precision_8: 0.7457\n",
      "Epoch 34/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5485 - precision_8: 0.7500\n",
      "Epoch 35/150\n",
      "217/217 [==============================] - ETA: 0s - loss: 0.5483 - precision_8: 0.747 - 2s 8ms/step - loss: 0.5470 - precision_8: 0.7463\n",
      "Epoch 36/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5534 - precision_8: 0.7398\n",
      "Epoch 37/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5500 - precision_8: 0.7483\n",
      "Epoch 38/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5493 - precision_8: 0.7418\n",
      "Epoch 39/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5451 - precision_8: 0.7506\n",
      "Epoch 40/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5475 - precision_8: 0.7419\n",
      "Epoch 41/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5452 - precision_8: 0.7491\n",
      "Epoch 42/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5446 - precision_8: 0.7487\n",
      "Epoch 43/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5424 - precision_8: 0.7594\n",
      "Epoch 44/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5446 - precision_8: 0.7453\n",
      "Epoch 45/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5404 - precision_8: 0.7530\n",
      "Epoch 46/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5433 - precision_8: 0.7484\n",
      "Epoch 47/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5401 - precision_8: 0.7547\n",
      "Epoch 48/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5365 - precision_8: 0.7535\n",
      "Epoch 49/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5414 - precision_8: 0.7627\n",
      "Epoch 50/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5383 - precision_8: 0.7498\n",
      "Epoch 51/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5384 - precision_8: 0.7453\n",
      "Epoch 52/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5423 - precision_8: 0.7411\n",
      "Epoch 53/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5401 - precision_8: 0.7560\n",
      "Epoch 54/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5390 - precision_8: 0.7581\n",
      "Epoch 55/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5407 - precision_8: 0.7551\n",
      "Epoch 56/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5326 - precision_8: 0.7535\n",
      "Epoch 57/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5352 - precision_8: 0.7570\n",
      "Epoch 58/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5397 - precision_8: 0.7591\n",
      "Epoch 59/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5346 - precision_8: 0.7471\n",
      "Epoch 60/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5334 - precision_8: 0.7563\n",
      "Epoch 61/150\n",
      "217/217 [==============================] - 3s 12ms/step - loss: 0.5309 - precision_8: 0.7587\n",
      "Epoch 62/150\n",
      "217/217 [==============================] - 3s 12ms/step - loss: 0.5301 - precision_8: 0.7659\n",
      "Epoch 63/150\n",
      "217/217 [==============================] - 2s 11ms/step - loss: 0.5271 - precision_8: 0.7555\n",
      "Epoch 64/150\n",
      "217/217 [==============================] - 2s 10ms/step - loss: 0.5367 - precision_8: 0.7509\n",
      "Epoch 65/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5357 - precision_8: 0.7546\n",
      "Epoch 66/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5332 - precision_8: 0.7581\n",
      "Epoch 67/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5278 - precision_8: 0.7549\n",
      "Epoch 68/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5309 - precision_8: 0.7613\n",
      "Epoch 69/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5339 - precision_8: 0.7588\n",
      "Epoch 70/150\n",
      "217/217 [==============================] - 2s 10ms/step - loss: 0.5294 - precision_8: 0.7554\n",
      "Epoch 71/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5322 - precision_8: 0.7523\n",
      "Epoch 72/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5269 - precision_8: 0.7479\n",
      "Epoch 73/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5310 - precision_8: 0.7548\n",
      "Epoch 74/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5284 - precision_8: 0.7583\n",
      "Epoch 75/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5306 - precision_8: 0.7588\n",
      "Epoch 76/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5297 - precision_8: 0.7600\n",
      "Epoch 77/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5330 - precision_8: 0.7486\n",
      "Epoch 78/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5285 - precision_8: 0.7605\n",
      "Epoch 79/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5298 - precision_8: 0.7523\n",
      "Epoch 80/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5284 - precision_8: 0.7522\n",
      "Epoch 81/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5284 - precision_8: 0.7592\n",
      "Epoch 82/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5259 - precision_8: 0.7605\n",
      "Epoch 83/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5316 - precision_8: 0.7520\n",
      "Epoch 84/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5264 - precision_8: 0.7567\n",
      "Epoch 85/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5254 - precision_8: 0.7590\n",
      "Epoch 86/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5298 - precision_8: 0.7566\n",
      "Epoch 87/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5240 - precision_8: 0.7541\n",
      "Epoch 88/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5192 - precision_8: 0.7680\n",
      "Epoch 89/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5286 - precision_8: 0.7611\n",
      "Epoch 90/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5238 - precision_8: 0.7585\n",
      "Epoch 91/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5255 - precision_8: 0.7620\n",
      "Epoch 92/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5216 - precision_8: 0.7602\n",
      "Epoch 93/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5182 - precision_8: 0.7613\n",
      "Epoch 94/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5248 - precision_8: 0.7558\n",
      "Epoch 95/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5241 - precision_8: 0.7587\n",
      "Epoch 96/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5272 - precision_8: 0.7547\n",
      "Epoch 97/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5210 - precision_8: 0.7583\n",
      "Epoch 98/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5217 - precision_8: 0.7611\n",
      "Epoch 99/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5205 - precision_8: 0.7542\n",
      "Epoch 100/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5228 - precision_8: 0.7600\n",
      "Epoch 101/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5230 - precision_8: 0.7584\n",
      "Epoch 102/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5206 - precision_8: 0.7570\n",
      "Epoch 103/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5189 - precision_8: 0.7642\n",
      "Epoch 104/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5199 - precision_8: 0.7614\n",
      "Epoch 105/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5233 - precision_8: 0.7554\n",
      "Epoch 106/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5217 - precision_8: 0.7599\n",
      "Epoch 107/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5152 - precision_8: 0.7599\n",
      "Epoch 108/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5197 - precision_8: 0.7586\n",
      "Epoch 109/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5207 - precision_8: 0.7569\n",
      "Epoch 110/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5194 - precision_8: 0.7638\n",
      "Epoch 111/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5200 - precision_8: 0.7587\n",
      "Epoch 112/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5196 - precision_8: 0.7564\n",
      "Epoch 113/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5225 - precision_8: 0.7501\n",
      "Epoch 114/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5221 - precision_8: 0.7609\n",
      "Epoch 115/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5216 - precision_8: 0.7638\n",
      "Epoch 116/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5236 - precision_8: 0.7636\n",
      "Epoch 117/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5198 - precision_8: 0.7584\n",
      "Epoch 118/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5179 - precision_8: 0.7630\n",
      "Epoch 119/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5206 - precision_8: 0.7549\n",
      "Epoch 120/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5204 - precision_8: 0.7537\n",
      "Epoch 121/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5209 - precision_8: 0.7604\n",
      "Epoch 122/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5181 - precision_8: 0.7685\n",
      "Epoch 123/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5209 - precision_8: 0.7595\n",
      "Epoch 124/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5169 - precision_8: 0.7672\n",
      "Epoch 125/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5173 - precision_8: 0.7656\n",
      "Epoch 126/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5151 - precision_8: 0.7691\n",
      "Epoch 127/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5200 - precision_8: 0.7624\n",
      "Epoch 128/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5099 - precision_8: 0.7703\n",
      "Epoch 129/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5140 - precision_8: 0.7629\n",
      "Epoch 130/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5119 - precision_8: 0.7652\n",
      "Epoch 131/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5174 - precision_8: 0.7666\n",
      "Epoch 132/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5104 - precision_8: 0.7725\n",
      "Epoch 133/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5194 - precision_8: 0.7656\n",
      "Epoch 134/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5171 - precision_8: 0.7661\n",
      "Epoch 135/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5180 - precision_8: 0.7658\n",
      "Epoch 136/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5187 - precision_8: 0.7639A: 0s - loss: 0.5179 - precision_8: 0.766\n",
      "Epoch 137/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5136 - precision_8: 0.7647\n",
      "Epoch 138/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5166 - precision_8: 0.7627\n",
      "Epoch 139/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5119 - precision_8: 0.7649\n",
      "Epoch 140/150\n",
      "217/217 [==============================] - 2s 8ms/step - loss: 0.5177 - precision_8: 0.7554\n",
      "Epoch 141/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5151 - precision_8: 0.7655\n",
      "Epoch 142/150\n",
      "217/217 [==============================] - ETA: 0s - loss: 0.5190 - precision_8: 0.760 - 2s 9ms/step - loss: 0.5193 - precision_8: 0.7608\n",
      "Epoch 143/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5159 - precision_8: 0.7690\n",
      "Epoch 144/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5178 - precision_8: 0.7697\n",
      "Epoch 145/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5173 - precision_8: 0.7733\n",
      "Epoch 146/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5151 - precision_8: 0.7658\n",
      "Epoch 147/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5172 - precision_8: 0.7642\n",
      "Epoch 148/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5153 - precision_8: 0.7660\n",
      "Epoch 149/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5133 - precision_8: 0.7727\n",
      "Epoch 150/150\n",
      "217/217 [==============================] - 2s 9ms/step - loss: 0.5179 - precision_8: 0.7639\n"
     ]
    }
   ],
   "source": [
    "# Create a new sequential model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Create a regularizer with a factor of 0.005 and apply it to all hidden layers\n",
    "regularizer = keras.regularizers.l2(0.005)\n",
    "\n",
    "# Declare the hidden layers and set the # of neurons in the first layer to roughly macth the number of input variables\n",
    "model.add(layers.Dense(500, activation=\"relu\", kernel_regularizer=regularizer))\n",
    "model.add(layers.Dropout(0.2)) # Set 20% of the nodes to 0. \n",
    "model.add(layers.Dense(250, activation=\"relu\", kernel_regularizer=regularizer))\n",
    "model.add(layers.Dropout(0.2)) # Set 20% of the nodes to 0. \n",
    "model.add(layers.Dense(125, activation=\"relu\", kernel_regularizer=regularizer))\n",
    "model.add(layers.Dropout(0.2)) # Set 20% of the nodes to 0. \n",
    "\n",
    "# Declare the output layer\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the model and leave learning rate as default. Set precision as the evaluation metric.\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[keras.metrics.Precision()]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_ohe_ss, \n",
    "                    y_train, \n",
    "                    epochs=150, \n",
    "                    verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Precision: 0.7639\n",
      "Test Precision: 0.7476\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the network\n",
    "train_precision = history.history['precision_8'][-1]\n",
    "result = model.evaluate(X_test_ohe_ss, y_test, verbose=0)\n",
    "\n",
    "print(f\"Train Precision: {train_precision:.4f}\")\n",
    "print(f\"Test Precision: {result[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(history.history['precision_8'], label = 'precision')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.ylim(0.5,0.8)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss seems to have leveled off after ~130 epoches and this model has achieved a test precision score of 0.75. This has been the best prediction score among all the classification models tested so far. \n",
    "\n",
    "While based on the performance, a 3-layer Feedforward Neural Network will be declared as the best classifier for the project, there are two things worth noting:\n",
    "- This encouraging initial results from a simple Neural Network warrant further optimization work. Only a very small set of hyperparameters have been tested so far: 2 architectures, 1 activation function, 1 regularization, 1 learning rate, and a few epoches. \n",
    "- Neural Network models are non-linear and lack interpretability. Prediction is the objective of this project, but being able to draw insights from the features is equally important. Through some literature review, I came across an evaluation framework called SHAP (SHapley Additive exPlanations) which was said to be able to help interpret Neural Network models. I have briefly tested it on my models with no success but will continue trying. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 38.1min\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "cachedir = mkdtemp()\n",
    "\n",
    "# create a function that returns a model\n",
    "\n",
    "def create_model(optimizer='adam',\n",
    "                 regularizer='l1',\n",
    "                 initializer='normal',\n",
    "                 dropout=0.2):\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # Declare the hidden layers and set the # of neurons in the first layer to roughly macth the number of input variables\n",
    "    model.add(layers.Dense(500, activation=\"relu\", kernel_regularizer=regularizer, kernel_initializer = initializer))\n",
    "    model.add(layers.Dropout(dropout))  \n",
    "    model.add(layers.Dense(250, activation=\"relu\", kernel_regularizer=regularizer, kernel_initializer = initializer))\n",
    "    model.add(layers.Dropout(dropout)) \n",
    "    model.add(layers.Dense(125, activation=\"relu\", kernel_regularizer=regularizer, kernel_initializer = initializer))\n",
    "    model.add(layers.Dropout(dropout)) \n",
    "\n",
    "    # Declare the output layer\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "    optimizer= optimizer,\n",
    "    loss=keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[keras.metrics.Precision()])\n",
    "\n",
    "    return model\n",
    "\n",
    "seed = 526\n",
    "np.random.seed(seed)\n",
    "\n",
    "# wrap the model using the function you created\n",
    "clf = KerasClassifier(build_fn=create_model,epochs = 50, verbose=0)\n",
    "\n",
    "# Set up the ML Pipeline\n",
    "\n",
    "# numerical features\n",
    "numeric_features = ['rrp', 'dollars_per_part','years_since_release', 'num_parts', 'is_parent_theme', 'num_minifigs',\n",
    "       'prop_trans_parts', 'num_unique_parts', 'num_part_cat', 'num_cat_part_count', 'review_score']\n",
    "numeric_transformer = StandardScaler()\n",
    "\n",
    "# categorical features\n",
    "categorical_features = ['theme_name', 'dominant_color', 'availability']\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers = [('num', numeric_transformer, numeric_features),\n",
    "                                                 ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# prediction pipeline\n",
    "\n",
    "pipeline = Pipeline(steps = [('preprocessor', preprocessor),\n",
    "                       ('clf', clf)])\n",
    "\n",
    "param_grid = {\n",
    "    'clf__optimizer':['rmsprop','adam','sgd'],\n",
    "    'clf__regularizer':['l1','l2'],\n",
    "    'clf__epochs':[50,100,150,200],\n",
    "    'clf__dropout':[0.1,0.2,0.3],\n",
    "    'clf__initializer':['glorot_uniform','normal','uniform']\n",
    "}\n",
    "\n",
    "# Set precision as the evaluation metric and perform a 5-fold validation\n",
    "grid = GridSearchCV(pipeline, param_grid=param_grid, scoring = 'precision',cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "fitted_grid = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid.best_score_, grid.best_params_))\n",
    "means = grid.cv_results_['mean_test_score']\n",
    "stds = grid.cv_results_['std_test_score']\n",
    "params = grid.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
